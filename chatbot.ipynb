{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# チャットボット作成  \n",
    "https://www.udemy.com/course/ai-nlp-bot/  \n",
    "(参照元)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャットボット作成の流れは、  \n",
    "①文字の読み込み  \n",
    "②文章のベクトル化  \n",
    "③返答用の関数  \n",
    "④チャットボットの構築  \n",
    "である。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスを作成する。  \n",
    "青空文庫より、宮沢賢治の作品を元に作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ①文字の読み込みのための準備  \n",
    "・青空文庫からデータを取得する。  \n",
    "・テキストの量は少ないと精度が悪くなり、多いと学習に時間を要する。よって10個のテキストによって学習を行う。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kenji_novelsというフォルダにテキストファイルを用意する。  \n",
    "青空文庫URL  \n",
    "https://www.aozora.gr.jp/index_pages/person81.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "novels = [\"gingatetsudono_yoru.txt\", \"serohikino_goshu.txt\", \"chumonno_oi_ryoriten.txt\",\n",
    "         \"gusukobudorino_denki.txt\", \"kaeruno_gomugutsu.txt\", \"kaino_hi.txt\", \"kashiwabayashino_yoru.txt\",\n",
    "         \"kazeno_matasaburo.txt\", \"kiirono_tomato.txt\", \"oinomorito_zarumori.txt\"]  # 青空文庫より\n",
    "\n",
    "text = \"\"\n",
    "for novel in novels:\n",
    "    with open(\"kenji_novels/\"+novel, mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
    "        text_novel = f.read()\n",
    "    text_novel = re.sub(\"《[^》]+》\", \"\", text_novel)  # ルビの削除\n",
    "    text_novel = re.sub(\"［[^］]+］\", \"\", text_novel)  # 読みの注意の削除\n",
    "    text_novel = re.sub(\"〔[^〕]+〕\", \"\", text_novel)  # 読みの注意の削除\n",
    "    text_novel = re.sub(\"[ 　\\n「」『』（）｜※＊…]\", \"\", text_novel)  # 全角半角スペース、改行、その他記号の削除\n",
    "    text += text_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキストをひらがな、カタカナ、記号のみにする。\n",
    "from pykakasi import kakasi\n",
    "\n",
    "text = text.replace(\"苹果\", \"ひょうか\")\n",
    "#pykakasiに載っていない漢字なので手動で平仮名に変換する。\n",
    "\n",
    "seperator = \"。\"  # 。をセパレータに指定\n",
    "sentence_list = text.split(seperator)  # セパレーターを使って文章をリストに分割する\n",
    "sentence_list.pop() # 最後の要素は空の文字列になるので、削除\n",
    "sentence_list = [x+seperator for x in sentence_list]  # 文章の最後に。を追加\n",
    "\n",
    "kakasi = kakasi()\n",
    "kakasi.setMode(\"J\", \"H\")  # J(漢字) からH(ひらがな)へ\n",
    "conv = kakasi.getConverter()\n",
    "\n",
    "kana_text = conv.do(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキストをファイルに保存する。\n",
    "with open(\"kana_kenji.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(kana_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "hiragana = \"ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞ\\\n",
    "ただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽ\\\n",
    "まみむめもゃやゅゆょよらりるれろゎわゐゑをん\"\n",
    "\n",
    "katakana = \"ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾ\\\n",
    "タダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポ\\\n",
    "マミムメモャヤュユョヨラリルレロヮワヰヱヲンヴ\"\n",
    "\n",
    "chars = hiragana + katakana\n",
    "\n",
    "with open(\"kana_kenji.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # 直前で保存したファイル\n",
    "    text = f.read()\n",
    "    \n",
    "for char in text:  # ひらがな、カタカナ以外でコーパスに使われている文字を追加\n",
    "    if char not in chars:\n",
    "        chars += char\n",
    "        \n",
    "chars += \"\\t\\n\"  # タブと改行を追加\n",
    "        \n",
    "chars_list = sorted(list(chars))  # 文字列をリストに変換してソートする\n",
    "\n",
    "with open(\"kana_chars.pickle\", mode=\"wb\") as f:  # pickleで保存\n",
    "    pickle.dump(chars_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kana_chars.pickleを作ったので、それを読み込む。\n",
    "with open('kana_chars.pickle', mode='rb') as f:\n",
    "    chars_list = pickle.load(f)\n",
    "\n",
    "def is_invalid(message):\n",
    "    is_invalid =False\n",
    "    for char in message:\n",
    "        if char not in chars_list:\n",
    "            is_invalid = True\n",
    "    return is_invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ②one-hot表現による、文章のベクトル化を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "    \n",
    "n_char = len(chars_list)\n",
    "max_length_x = 128\n",
    "\n",
    "# 文章をone-hot表現に変換する関数\n",
    "def sentence_to_vector(sentence):\n",
    "    vector = np.zeros((1, max_length_x, n_char), dtype=np.bool)\n",
    "    for j, char in enumerate(sentence):\n",
    "        vector[0][j][char_indices[char]] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ③返答用の関数を設定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)学習モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}  # 文字がキーでインデックスが値\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}  # インデックスがキーで文字が値\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "    \n",
    "seperator = \"。\"\n",
    "sentence_list = text.split(seperator) \n",
    "sentence_list.pop() \n",
    "sentence_list = [x+seperator for x in sentence_list]\n",
    "\n",
    "max_sentence_length = 128  # 文章の最大長さ。これより長い文章はカットされる。\n",
    "sentence_list = [sentence for sentence in sentence_list if len(sentence) <= max_sentence_length]  # 長すぎる文章のカット\n",
    "\n",
    "n_char = len(chars_list)  # 文字の種類の数\n",
    "n_sample = len(sentence_list) - 1  # サンプル数\n",
    "\n",
    "x_sentences = []  # 入力の文章\n",
    "t_sentences = []  # 正解の文章\n",
    "for i in range(n_sample):\n",
    "    x_sentences.append(sentence_list[i])\n",
    "    t_sentences.append(\"\\t\" + sentence_list[i+1] + \"\\n\")  # 正解は先頭にタブ、末尾に改行を加える\n",
    "max_length_x = max_sentence_length  # 入力文章の最大長さ\n",
    "max_length_t = max_sentence_length + 2  # 正解文章の最大長さ\n",
    "\n",
    "x_encoder = np.zeros((n_sample, max_length_x, n_char), dtype=np.bool)  # encoderへの入力\n",
    "x_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderへの入力\n",
    "t_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderの正解\n",
    "\n",
    "for i in range(n_sample):\n",
    "    x_sentence = x_sentences[i]\n",
    "    t_sentence = t_sentences[i]\n",
    "    for j, char in enumerate(x_sentence):\n",
    "        x_encoder[i, j, char_indices[char]] = 1  # encoderへの入力をone-hot表現で表す\n",
    "    for j, char in enumerate(t_sentence):\n",
    "        x_decoder[i, j, char_indices[char]] = 1  # decoderへの入力をone-hot表現で表す\n",
    "        if j > 0:  # 正解は入力より1つ前の時刻のものにする\n",
    "            t_decoder[i, j-1, char_indices[char]] = 1\n",
    "            \n",
    "print(x_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, GRU, Input, Masking\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "n_mid = 256  # 中間層のニューロン数\n",
    "\n",
    "encoder_input = Input(shape=(None, n_char))\n",
    "encoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "encoder_masked = encoder_mask(encoder_input)\n",
    "encoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_state=True)  # dropoutを設定し、ニューロンをランダムに無効にする\n",
    "encoder_output, encoder_state_h = encoder_lstm(encoder_masked)\n",
    "\n",
    "decoder_input = Input(shape=(None, n_char))\n",
    "decoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "decoder_masked = decoder_mask(decoder_input)\n",
    "decoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True)  # dropoutを設定\n",
    "decoder_output, _ = decoder_lstm(decoder_masked, initial_state=encoder_state_h)  # encoderの状態を初期状態にする\n",
    "decoder_dense = Dense(n_char, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "#val_lossに改善が見られなくなってから、30エポックで学習は終了\n",
    "#学習に時間を要するので、終了するエポック数は減らしても良いかもしれない。\n",
    "#下記のコードでは124回行って終了した。\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=30) \n",
    "\n",
    "history = model.fit([x_encoder, x_decoder], t_decoder,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     validation_split=0.1,  # 10%は検証用\n",
    "                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)予測用モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderのモデル\n",
    "encoder_model = Model(encoder_input, encoder_state_h)\n",
    "\n",
    "# decoderのモデル\n",
    "decoder_state_in_h = Input(shape=(n_mid,))\n",
    "decoder_state_in = [decoder_state_in_h]\n",
    "\n",
    "decoder_output, decoder_state_h = decoder_lstm(decoder_input,\n",
    "                                               initial_state=decoder_state_in_h)\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model([decoder_input] + decoder_state_in,\n",
    "                      [decoder_output, decoder_state_h])\n",
    "\n",
    "# モデルの保存\n",
    "encoder_model.save('encoder_model.h5')\n",
    "decoder_model.save('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)返答用関数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "encoder_model = load_model('encoder_model.h5')\n",
    "decoder_model = load_model('decoder_model.h5')\n",
    "\n",
    "def respond(message, beta=5):\n",
    "    vec = sentence_to_vector(message)  # 文字列をone-hot表現に変換\n",
    "    state_value = encoder_model.predict(vec)\n",
    "    y_decoder = np.zeros((1, 1, n_char))  # decoderの出力を格納する配列\n",
    "    y_decoder[0][0][char_indices['\\t']] = 1  # decoderの最初の入力はタブ。one-hot表現にする。\n",
    "\n",
    "    respond_sentence = \"\"  # 返答の文字列\n",
    "    while True:\n",
    "        y, h = decoder_model.predict([y_decoder, state_value])\n",
    "        p_power = y[0][0] ** beta  # 確率分布の調整\n",
    "        next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power)) \n",
    "        next_char = indices_char[next_index]  # 次の文字\n",
    "        \n",
    "        if (next_char == \"\\n\" or len(respond_sentence) >= max_length_x):\n",
    "            break  # 次の文字が改行のとき、もしくは最大文字数を超えたときは終了\n",
    "            \n",
    "        respond_sentence += next_char\n",
    "        y_decoder = np.zeros((1, 1, n_char))  # 次の時刻の入力\n",
    "        y_decoder[0][0][next_index] = 1\n",
    "\n",
    "        state_value = h  # 次の時刻の状態\n",
    "\n",
    "    return respond_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ④チャットボットの構築を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_name = \"賢治bot\"\n",
    "your_name = input(\"おなまえをおしえてください。:\")\n",
    "print()\n",
    "\n",
    "print(bot_name + \": \" + \"こんにちは、\" + your_name + \"さん。\")\n",
    "message = \"\"\n",
    "while message != \"さようなら。\":\n",
    "    \n",
    "    while True:\n",
    "        message = input(your_name + \": \")\n",
    "        if not is_invalid(message):\n",
    "            break\n",
    "        else:\n",
    "            print(bot_name + \": ひらがなか、カタカナをつかってください。\")\n",
    "            \n",
    "    response = respond(message)\n",
    "    print(bot_name + \": \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(課題)  \n",
    "返答の精度が悪い点。  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
